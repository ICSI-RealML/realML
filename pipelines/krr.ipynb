{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline for realML.kernel.RFMPreconditionedGaussianKRR and realML.kernel.RFMPreconditionedPolynomialKRR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "converted from the example pipeline for the r_26 radon prediction dataset: expects the `r_26` directory to be in the working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.model_selection import cross_val_score, ShuffleSplit, cross_validate\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from realML.kernel import RFMPreconditionedGaussianKRR, RFMPreconditionedPolynomialKRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data\n",
    "rootDir = './r_26'\n",
    "dataDir = os.path.join(rootDir,'data')\n",
    "assert os.path.exists(dataDir)\n",
    "rawDir = os.path.join(dataDir,'raw_data')\n",
    "assert os.path.exists(rawDir)\n",
    "\n",
    "trainData = pd.read_csv(os.path.join(dataDir, 'trainData.csv'), index_col=0)\n",
    "X_train = pd.read_csv(os.path.join(rawDir, 'radon_train.csv'), index_col=0)\n",
    "y_train = pd.read_csv(os.path.join(dataDir, 'trainTargets.csv'), index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually select the following two features ['county', 'floor']\n",
    "# selected by reading the problem and data descriptions\n",
    "X_train = X_train[['county', 'floor']]\n",
    "\n",
    "# both county and floor are categorical variables, requires one-hot encoding\n",
    "X_train = pd.get_dummies(X_train, columns = ['county', 'floor'])\n",
    "#print(X_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71476255733869487"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using the primitives directly (kind of useless, \n",
    "# since the new API means we can't use cross validate to check the model,\n",
    "# or grid search to find the parameters,\n",
    "# but informative)\n",
    "gaussKRR = RFMPreconditionedGaussianKRR(lparam = 1.0, sigma=1/np.sqrt(2*.1))\n",
    "gaussKRR.set_training_data(inputs=[X_train.as_matrix()], outputs=[y_train.as_matrix()])\n",
    "gaussKRR.fit()\n",
    "ypred = gaussKRR.produce(inputs=[X_train.as_matrix()])[0]\n",
    "np.linalg.norm(ypred - y_train.as_matrix())/np.sqrt(len(ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create wrapper sklearn estimator classes so can use cross_validate\n",
    "class gaussKRREstimator(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, lparam=1, sigma=1):\n",
    "        self.fastEstimator = RFMPreconditionedGaussianKRR(lparam=1, sigma=1)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.fastEstimator.set_training_data(inputs=[X.as_matrix()], outputs=[y.as_matrix()])\n",
    "        self.fastEstimator.fit()\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.fastEstimator.produce(inputs=[X.as_matrix()])[0]\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        return {\"lparam\": self.fastEstimator.lparam, \n",
    "                \"sigma\": self.fastEstimator.sigma}\n",
    "\n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self.fastEstimator, parameter, value)\n",
    "        return self\n",
    "    \n",
    "class polyKRREstimator(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, lparam=1, offset = 0, sf = 1):\n",
    "        self.fastEstimator = RFMPreconditionedPolynomialKRR(lparam = 1, offset = 0, sf = 1)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.fastEstimator.set_training_data(inputs=[X.as_matrix()], outputs=[y.as_matrix()])\n",
    "        self.fastEstimator.fit()\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.fastEstimator.produce(inputs=[X.as_matrix()])[0]\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        return {\"lparam\": self.fastEstimator.lparam, \n",
    "                \"offset\": self.fastEstimator.offset,\n",
    "                \"sf\": self.fastEstimator.sf}\n",
    "\n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self.fastEstimator, parameter, value)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1268.8624827 1319.75073404\n"
     ]
    }
   ],
   "source": [
    "gaussKRR = gaussKRREstimator(lparam=1.0, sigma=1/np.sqrt(2*(0.1)))\n",
    "gaussKRR.fit(X_train, y_train)\n",
    "predy = gaussKRR.predict(X_train)\n",
    "\n",
    "polyKRR = polyKRREstimator(lparam=1.0, offset=1.0, sf=1)\n",
    "polyKRR.fit(X_train, y_train)\n",
    "predy = gaussKRR.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying model: Gaussian kernel ridge...\n",
      "model performance on 10-fold CV (mean rmse) 0.761491708167\n",
      "gaussKRREstimator(lparam=0.01, sigma=21.544346900318846)\n"
     ]
    }
   ],
   "source": [
    "# do cross validation to select best gaussKRR fit\n",
    "RMSE = lambda yT, yP: np.sqrt(mean_squared_error(yT, yP))\n",
    "\n",
    "print('trying model: Gaussian kernel ridge...')\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.25, random_state=0)\n",
    "kr = GridSearchCV(\n",
    "    gaussKRREstimator(lparam=1.0, sigma=0.1), \n",
    "    cv=cv,\n",
    "    param_grid={\"lparam\": [1e0, 0.1, 1e-2, 1e-3],\n",
    "                \"sigma\": np.logspace(-2, 3, 7)},\n",
    "    scoring=make_scorer(RMSE, greater_is_better=False)\n",
    ")\n",
    "kr.fit(X_train, y_train)\n",
    "score = kr.best_score_ # that score is negative MSE scores. The thing is that GridSearchCV, by convention, always tries to maximize its score so loss functions like MSE have to be negated.\n",
    "score = score*-1\n",
    "print('model performance on 10-fold CV (mean rmse)', score)\n",
    "print(kr.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying model: Polynomial kernel ridge...\n",
      "model performance on 10-fold CV (mean rmse) 0.760978877808\n",
      "polyKRREstimator(lparam=0.001, offset=0.068129206905796158, sf=0.01)\n"
     ]
    }
   ],
   "source": [
    "# do cross validation to select best polyKRR fit\n",
    "RMSE = lambda yT, yP: np.sqrt(mean_squared_error(yT, yP))\n",
    "\n",
    "print('trying model: Polynomial kernel ridge...')\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.25, random_state=0)\n",
    "kr = GridSearchCV(\n",
    "    polyKRREstimator(lparam=1.0, offset=0.1, sf=1.0), \n",
    "    cv=cv,\n",
    "    param_grid={\"lparam\": [1e0, 0.1, 1e-2, 1e-3],\n",
    "                \"offset\": np.logspace(-2, 3, 7),\n",
    "                \"sf\": np.logspace(-2,3,7)},\n",
    "    scoring=make_scorer(RMSE, greater_is_better=False)\n",
    ")\n",
    "kr.fit(X_train, y_train)\n",
    "score = kr.best_score_ # that score is negative MSE scores. The thing is that GridSearchCV, by convention, always tries to maximize its score so loss functions like MSE have to be negated.\n",
    "score = score*-1\n",
    "print('model performance on 10-fold CV (mean rmse)', score)\n",
    "print(kr.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
